# Importar as bibliotecas necessárias
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# Definir o modelo sequencial usando PyTorch
class BetaVAE(nn.Module):
  def __init__(self, input_shape, latent_dim, beta):
    super(BetaVAE, self).__init__()
    self.input_shape = input_shape # (n_frames, width, height, channels)
    self.latent_dim = latent_dim # dimensão do vetor latente
    self.beta = beta # fator de regularização da divergência KL

    # Definir a camada ConvLSTM2D com filtros, tamanho do kernel e forma de entrada adequados para o problema
    self.conv_lstm = nn.ConvLSTM2D(input_size=(40, 40), input_dim=1, hidden_dim=40, kernel_size=(3, 3), num_layers=4)

    # Definir a camada de normalização em lote para melhorar o desempenho do modelo
    self.batch_norm = nn.BatchNorm2d(40)

    # Definir a camada linear para transformar a saída da ConvLSTM2D em um vetor latente de dimensão desejada
    self.fc1 = nn.Linear(40 * 40 * 40, latent_dim * 2) # multiplicamos por 2 porque vamos gerar média e log desvio padrão

    # Definir a camada linear para transformar o vetor latente em uma saída reconstruída com a mesma forma da entrada
    self.fc2 = nn.Linear(latent_dim, 40 * 40 * 40)

    # Definir a camada Conv3D com filtro, tamanho do kernel e ativação adequados para gerar a saída desejada
    self.conv3d = nn.Conv3D(in_channels=40, out_channels=1, kernel_size=(3, 3, 3), padding=(1, 1, 1), activation='sigmoid')

  def encode(self, x):
    # Codificar a entrada x em um vetor latente z
    x = self.conv_lstm(x) # aplicar a camada ConvLSTM2D
    x = self.batch_norm(x) # aplicar a camada de normalização em lote
    x = x.view(-1, 40 * 40 * 40) # achatar a saída em um vetor unidimensional
    x = self.fc1(x) # aplicar a camada linear
    mu = x[:, :self.latent_dim] # obter a média do vetor latente
    log_var = x[:, self.latent_dim:] # obter o log desvio padrão do vetor latente
    return mu, log_var

  def reparameterize(self, mu, log_var):
    # Reparametrizar o vetor latente usando uma distribuição normal padrão e a média e o log desvio padrão obtidos na codificação
    std = torch.exp(0.5 * log_var) # calcular o desvio padrão
    eps = torch.randn_like(std) # gerar um ruído aleatório com a mesma forma que o desvio padrão
    z = mu + eps * std # obter o vetor latente z
    return z

  def decode(self, z):
    # Decodificar o vetor latente z em uma saída reconstruída x_hat
    x_hat = self.fc2(z) # aplicar a camada linear inversa
    x_hat = x_hat.view(-1, 40, 40, 40) # remodelar o vetor em uma matriz tridimensional
    x_hat = self.conv3d(x_hat) # aplicar a camada Conv3D
    return x_hat

  def forward(self, x):
    # Executar o modelo completo: codificar -> reparametrizar -> decodificar
    mu, log_var = self.encode(x) # codificar a entrada x em média e log desvio padrão
    z = self.reparameterize(mu, log_var) # reparametrizar o vetor latente z usando média e log desvio padrão
    x_hat = self.decode(z) # decodificar o vetor latente z em uma saída reconstruída x_hat
    return x_hat, mu, log_var

  def loss_function(self, x, x_hat, mu, log_var):
    # Definir a função de perda que combina a perda de reconstrução e a perda de divergência KL
    recon_loss = nn.MSELoss(reduction='sum')(x_hat, x) # calcular a perda de reconstrução usando o erro quadrático médio
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) # calcular a perda de divergência KL usando a fórmula analítica
    loss = recon_loss + self.beta * kl_loss # combinar as duas perdas com um fator de regularização beta
    return loss

# Definir o otimizador (por exemplo, Adam) e um número de épocas para treinar o modelo
optimizer = optim.Adam(model.parameters(), lr=0.001) # usar o otimizador Adam com uma taxa de aprendizado de 0.001
epochs = 100 # definir o número de épocas para treinar o modelo

# Treinar o modelo com os dados da UC Berkeley milling data, usando uma memória de repetição para armazenar as transições entre estados, ações e recompensas, e usando uma rede neural alvo para estabilizar o aprendizado
# Carregar os dados da UC Berkeley milling data
data = np.load('milling_data.npy') # carregar os dados como um array numpy
data = torch.from_numpy(data).float() # converter os dados em um tensor do PyTorch
data = data.unsqueeze(-1) # adicionar uma dimensão extra para os canais

# Dividir os dados em conjuntos de treino e teste
train_data = data[:80] # usar os primeiros 80 casos como dados de treino
test_data = data[80:] # usar os últimos 20 casos como dados de teste

# Definir uma memória de repetição para armazenar as transições entre estados, ações e recompensas
memory_size = 10000 # definir o tamanho da memória de repetição
memory = [] # inicializar a memória como uma lista vazia

# Definir uma rede neural alvo para estabilizar o aprendizado
target_model = BetaVAE(input_shape=(15, 40, 40, 1), latent_dim=10, beta=4) # criar um modelo idêntico ao original
target_model.load_state_dict(model.state_dict()) # copiar os pesos do modelo original para o modelo alvo
target_model.eval() # colocar o modelo alvo em modo de avaliação

# Definir uma função para obter uma ação a partir de um estado usando a rede neural alvo
def get_action(state):
  # Usar a rede neural alvo para gerar uma saída reconstruída e um vetor latente a partir do estado atual
  with torch.no_grad(): # desativar o cálculo do gradiente para acelerar a inferência
    x_hat, mu, log_var = target_model(state)
  # Calcular a recompensa como uma função do desgaste da ferramenta de corte usando a fórmula do artigo
  reward = torch.exp(-0.5 * torch.sum(mu.pow(2) + log_var.exp(), dim=1))
  # Escolher a ação que maximiza a recompensa usando um método guloso
  action = torch.argmax(reward).item()
  return action

# Definir uma função para executar uma etapa do treinamento usando o otimizador e a função de perda do modelo original
def train_step():
  # Verificar se há dados suficientes na memória de repetição
  if len(memory) < batch_size:
    return
  # Amostrar um lote de transições da memória de repetição
  batch = random.sample(memory, batch_size)
  # Separar o lote em estados, ações, recompensas e próximos estados
  states = torch.stack([item[0] for item in batch])
  actions = torch.stack([item[1] for item in batch])
  rewards = torch.stack([item[2] for item in batch])
  next_states = torch.stack([item[3] for item in batch])
  # Usar o modelo original para gerar as saídas reconstruídas e os vetores latentes dos estados e dos próximos estados
  x_hat, mu, log_var = model(states)
  next_x_hat, next_mu, next_log_var = model(next_states)
  # Calcular a perda de reconstrução usando a função de perda do modelo original
  recon_loss = model.loss_function(x_hat, states, mu, log_var)
  # Calcular a perda de reforço usando a fórmula do artigo
  q_values = torch.exp(-0.5 * torch.sum(mu.pow(2) + log_var.exp(), dim=1)) # calcular os valores Q para os estados atuais
  next_q_values = torch.exp(-0.5 * torch.sum(next_mu.pow(2) + next_log_var.exp(), dim=1)) # calcular os valores Q para os próximos estados
  max_next_q_values = torch.max(next_q_values) # obter o valor Q máximo para os próximos estados
  expected_q_values = rewards + gamma * max_next_q_values # calcular os valores Q esperados usando a equação de Bellman
  reinforce_loss = nn.MSELoss()(q_values.gather(1, actions.unsqueeze(1)), expected_q_values.unsqueeze(1)) # calcular a perda de reforço usando o erro quadrático médio entre os valores Q e os valores Q esperados
  # Combinar as duas perdas com um fator de ponderação alpha
  loss = alpha * recon_loss + (1 - alpha) * reinforce_loss
  # Atualizar os pesos do modelo original usando o otimizador
  optimizer.zero_grad() # zerar os gradientes acumulados
  loss.backward() # calcular os gradientes da perda em relação aos pesos
  optimizer.step() # atualizar os pesos usando os gradientes
  return loss.item() # retornar o valor da perda

# Definir alguns hiperparâmetros para o treinamento
batch_size = 32 # tamanho do lote para amostrar da memória de repetição
gamma = 0.99 # fator de desconto para a equação de Bellman
alpha = 0.5 # fator de ponderação para combinar as perdas de reconstrução e reforço
update_freq = 1000 # frequência de atualização do modelo alvo em relação ao modelo original

# Inicializar algumas variáveis para armazenar as métricas do treinamento
train_losses = [] # lista para armazenar as perdas de treinamento
test_losses = [] # lista para armazenar as perdas de teste
test_ruls = [] # lista para armazenar as previsões de RUL nos dados de teste
test_tcms = [] # lista para armazenar as previsões de TCM nos dados de teste

# Loop principal do treinamento
for epoch in range(epochs):
  # Treinar o modelo em cada caso do conjunto de treino
  for i in range(len(train_data)):
    case = train_data[i] # obter o caso atual
    state = case[:15] # obter o estado atual como os primeiros 15 frames do caso
    for t in range(15, len(case)):
      action = get_action(state) # obter a ação usando a rede neural alvo
      reward = case[t][-1][0][0] # obter a recompensa como o último pixel do frame atual (que representa o desgaste da ferramenta)
      next_state = case[t-14:t+1] # obter o próximo estado como os próximos 15 frames do caso
      memory.append((state, action, reward, next_state)) # armazenar a transição na memória de repetição
      state = next_state # atualizar o estado atual para o próximo estado
      train_loss = train_step() # executar uma etapa do treinamento usando o otimizador e a função de perda do modelo original
      train_losses.append(train_loss) # armazenar a perda de treinamento na lista
      if len(memory) % update_freq == 0: # verificar se é hora de atualizar o modelo alvo
        target_model.load_state_dict(model.state_dict()) # copiar os pesos do modelo original para o modelo alvo
  # Avaliar o modelo em cada caso do conjunto de teste
  for j in range(len(test_data)):
    case = test_data[j] # obter o caso atual
    state = case[:15] # obter o estado atual como os primeiros 15 frames do caso
    test_loss = 0 # inicializar a perda de teste como zero
    test_rul = 0 # inicializar a previsão de RUL como zero
    test_tcm = 0 # inicializar a previsão de TCM como zero
    for t in range(15, len(case)):
      with torch.no_grad(): # desativar o cálculo do gradiente para acelerar a inferência
        x_hat, mu, log_var = model(state) # usar o modelo original para gerar as saídas reconstruídas e os vetores latentes dos estados atuais
        test_loss += model.loss_function(x_hat, state, mu, log_var) # acumular a perda de teste usando a função de perda do modelo original
        q_values = torch.exp(-0.5 * torch.sum(mu.pow(2) + log_var.exp(), dim=1)) # calcular os valores Q para os estados atuais
        action = torch.argmax(q_values).item() # escolher a ação que maximiza os valores Q usando um método guloso
        reward = case[t][-1][0][0] # obter a recompensa como o último pixel do frame atual (que representa o desgaste da ferramenta)
        next_state = case[t-14:t+1] # obter o próximo estado como os próximos 15 frames do caso
        state = next_state # atualizar o estado atual para o próximo estado
        if reward < 0.5: # verificar se a ferramenta está em um estado de falha (recompensa menor que 0.5)
          test_rul = t - 15 # calcular a previsão de RUL como o tempo até a falha da ferramenta
          test_tcm = action + 1 # calcular a previsão de TCM como a ação escolhida mais um (para corresponder aos níveis de TCM definidos no artigo)
          break # interromper o loop interno
    test_losses.append(test_loss.item()) # armazenar a perda de teste na lista
    test_ruls.append(test_rul) # armazenar a previsão de RUL na lista
    test_tcms.append(test_tcm) # armazenar a previsão de TCM na lista

  # Imprimir as métricas do treinamento e do teste após cada época
  print(f'Epoch {epoch + 1}')
  print(f'Train loss: {np.mean(train_losses)}')
  print(f'Test loss: {np.mean(test_losses)}')
  print(f'Test RUL: {np.mean(test_ruls)}')
  print(f'Test TCM: {np.mean(test_tcms)}')

# Plotar as curvas de perda de treinamento e de teste
plt.plot(train_losses, label='Train loss')
plt.plot(test_losses, label='Test loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plotar as previsões de RUL e TCM nos dados de teste
plt.plot(test_ruls, label='RUL')
plt.plot(test_tcms, label='TCM')
plt.xlabel('Case')
plt.ylabel('Prediction')
plt.legend()
plt.show()

# Comparar as previsões de RUL e TCM com outros métodos existentes na literatura
# Carregar os resultados dos outros métodos como arrays numpy
rul_svm = np.load('rul_svm.npy') # resultados do método SVM
rul_lstm = np.load('rul_lstm.npy') # resultados do método LSTM
tcm_svm = np.load('tcm_svm.npy') # resultados do método SVM
tcm_lstm = np.load('tcm_lstm.npy') # resultados do método LSTM

# Calcular as métricas de avaliação para cada método
# Usar o erro absoluto médio (MAE) para o RUL e a acurácia para o TCM
mae_rul_beta_vae = np.mean(np.abs(test_ruls - rul_true)) # MAE do beta-VAE para o RUL
mae_rul_svm = np.mean(np.abs(rul_svm - rul_true)) # MAE do SVM para o RUL
mae_rul_lstm = np.mean(np.abs(rul_lstm - rul_true)) # MAE do LSTM para o RUL

acc_tcm_beta_vae = np.mean(test_tcms == tcm_true) # acurácia do beta-VAE para o TCM
acc_tcm_svm = np.mean(tcm_svm == tcm_true) # acurácia do SVM para o TCM
acc_tcm_lstm = np.mean(tcm_lstm == tcm_true) # acurácia do LSTM para o TCM

# Imprimir as métricas de avaliação para cada método
print(f'MAE of beta-VAE for RUL: {mae_rul_beta_vae}')
print(f'MAE of SVM for RUL: {mae_rul_svm}')
print(f'MAE of LSTM for RUL: {mae_rul_lstm}')

print(f'Accuracy of beta-VAE for TCM: {acc_tcm_beta_vae}')
print(f'Accuracy of SVM for TCM: {acc_tcm_svm}')
print(f'Accuracy of LSTM for TCM: {acc_tcm_lstm}')

# Salvar o modelo treinado
torch.save(model.state_dict(), 'beta_vae_model.pth') # salvar os pesos do modelo original
torch.save(target_model.state_dict(), 'beta_vae_target_model.pth') # salvar os pesos do modelo alvo
